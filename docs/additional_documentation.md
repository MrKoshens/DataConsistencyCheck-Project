# Additional Documentation

## Date Columns

It is recommended that the pandas dataframe passed has any date columns set such that the dtype is datetime64 or timestamp. However, the tool will attempt to convert any columns with dtype as object to datetime, and will treat any that can be successfully converted as datetime columns, which means all tests related to date or time values will be executed on these columns. If verbose is set to 2 or higher, the column data types used will be displayed at the top ot the output running check_data_quality(). It is also possible to call display_columns_types_list() or display_columns_types_table() to check the data types that were inferred for the passed dataframe. If desired, you may pass a list of date columns to init_data() and the tool will attempt to treat all of these, and only these, columns as datetime columns. 

## Data Dredging and False Postives

This tool runs a large number of tests, some of which check many sets of columns. For example, the test to check if one column tends to be the sum of another set of columns, can check a very large number of subsets of the full set of numeric columns. This can lead to false positives, where patterns are reported that are random, or not meaningful. Ideally the output is clear enough that any patterns found may be confirmed, and checked to determine how relevant they are. A key property of the tool, though, is that each row is tested many times, and so while there may be some false positives reported, rows flagged many times, particularly those reported many times for different tests on different columns, are certainly unusual. Note though, reliably identifying rows as unusual does not suggest anything regarding whether the rows are correct or if they are useful, only that they are different from the other rows in the same dataset. 

Where there are erroneous patterns, it often occurs because the tests compare two or more columns that are not really comparable, though the algorithm has no way to intuit this. This is an issue with all outlier detection tools, which treat all columns as being directly comparable. DataConsistencyChecker does have the advantage that it makes these cases plain, and allows calling clear_issues() to remove these where desired. 

## Internal Parameters

As with all outlier detectors, there are some thresholds and other decisions necessary to define what constitutes a pattern and what constitutes an exception to the pattern. There are options to configure the tests, such as the coefficients used for interquartile range and interdecile range tests. For the most part, though, we have taken the approach of trying to limit options, and to provide clear, sensible tests, without the need to experiment or run the tool multiple times. However, almost all tests could, at least potentially, be tweaked to run with different thresholds or with different options, which would produce different output. For example, one test exists to check if, for sets of numeric columns, if the values in these columns, row for row, tend to all be positive or all negative together. In this case, there is a decision how to handle zero values. While such decisions are made based on testing on a very large number of datasets, other decisions could be made. We have attempted in all cases to make the most reasonable decisions and, though alternative, similar tests may also be useful, the output here has been confirmed to be accurate and useful.

As another example, there are tests to determine if the values in one column can be predicted from the values in the other columns using a small decision tree or linear regression. In these cases, there are decisions related, for example, to the minimum accuracies of the models. As such, these tests should be taken simply as presenting the patterns it does find, without any statement about other patterns that could be found were different options used. Configuring these options does invite running through many combinations of options, increases data dredging and producing less combrehensible output. 

## Clearing Issues

All outlier detectors, including DataConsistencyChecker, allow some tuning. Detectors that return binary labels further require some thresholds, but all provide some options. For example, kth Nearest Neighbors requires defining k, a distance metric, a means of scaling each numeric column, a method of encoding categorical features, and so on. This is true of DataQualityChecker as well, though it has the advantage of executing many tests, such that any decisions made on individual tests will tend to average out over many tests, therefore requiring less tuning, though some tuning is possible if desired. Primarily though, we recommend tuning in the form of clearing issues, such that analysis begins with the full set of issues identified, followed by some manual pruning of any issues considered inconsequental, leaving the remaining issues, and a final outlier score per row. 

A set of APIs, clear_results(), and restore_results() are provided to allow users to examine the results, and incrementally clear any issues found which may be considered non-issues, leaving a final set of patterns and exceptions that are consisidered noteworthy. This can help reduce some noise when a non-trivial number of issues are found, or when reporting issues to others. 

An example is provided in [clearing issues demo notebook](https://github.com/Brett-Kennedy/DataConsistencyChecker/blob/main/Demo%20Notebooks/Demo_Clear_Issues.ipynb)

## Contamination Level

While exceptions to patterns necessarily require decisions related to thresholds, patterns without exceptions are more straight-forward. In most cases, the pattern clearly exists or does not. For example a numeric column is monotonically increasing or it is not; a string column consistently contains 4 uppercase characters or it does not. These are typically clear, and can be quite straightforward to assess. With exceptions, however, some decisions must be made about the nature of the exceptions, but in most cases, this is simply a case of specifiying the number of exceptions permitted such that we recognize a pattern as still existing, albeit with exceptions. 

In general, much of tuning outlier detectors is in the form of determining how many outliers should be flagged. With DataConsistencyChecker, this is done most simply with the contamination_level parameter, which defines the maximum number or exceptions to allow to a pattern for it to be considered a pattern with exceptions, as opposed to a non-pattern. This may be specified as an absolute count or as a fraction of the total number of rows. The default is set to 0.5%, which we have found quite sensible, but tuning this may be quite reasonable as well. 

If there are, say, 10,000 rows in a dataset, then the default value of 0.5% exceptions to any pattern allows here 50 rows. For example, one test checks if two columns have values such that one is the result of rounding the other. In this example, if this is true in all 10,000 rows, this will be identified as a pattern with no exceptions; if this is true in all but 1 to 50 rows, this will be considered a pattern with exceptions, and if false in 51 or more rows, it will not be considered a pattern. Adjusting the contamination_level up or down can provide a more suitable number of exceptions for some cases. 

In most cases, where tuning is necessary, tuning this single parameter will be sufficient. However, it you wish to identify more outliers than have been flagged, it is also possible to increase max_combinations, which will allow examining of more combinations of features for some tests. 

## Noting the Patterns Not Found

DataConsistencyChecker performs sufficient tests that it's reasonably likely to find something of interest in any non-trivial dataset and, where it does not, there is real meaning in the fact that it does not -- this suggests a very consistent dataset. 

In general, what it does not flag can be of interest as well as what it does flag. For example, any rows with scores of zero may be reasonably considered to be very normal with respect to the dataset. As well, the tool provides interfaces to identify where patterns were not present. For example, if it may be expected that a column contains entirely positive values, then it is possible to see that it is not the case that this column contains entirely positive values (the POSITIVE test did not identify a pattern for this column), or that it contains almost entirely positive values, with some exceptions.  

## Types of Patterns Presented
We may look at each test as covering one of three cases related to patterns: 1) The test may identify patterns that do not occur in most datasets and are interesting in themselves, even where no exceptions are found, for example where one column is found to be the sum of two or more other columns, or two columns have identical values. 2) The test may identify patterns that are common in datasets, and may only be interesting if there are exceptions, for example finding that all numeric values are positive, or that all string values are strictly in lowercase. These, by default, are not returned in most API calls, though setting show_short_list_only to False for the relevant APIs will present these patterns as well. 3) Some tests have no concept of patterns, for example, tests to find very small or very large values; these tests return, if there are any results, only exceptions. All tests may return exceptions. 

## Sort Order for Examined Data
Where applicable, the dataframe should be sorted before running any tests. In many cases, there is no natural order to the data, but where there is, several of the tests check for patterns such as columns containing consistently increasing or decreasing values, repeating patterns in columns, and so on. Where the data is, for example, collected from SQL before running DataConsistencyChecker, the row order may other than the natural order of the data, and sorting by the relevant columns first will be useful.

## Code / ID values
Some tests on string columns are specific to columns with code or ID values, such that the individual characters in the values may have meaning. For example, with value X7333, it may be relevant that the first character is an 'X', or that the subsequent characters are 4 numeric characters. Or if two columns in a row have values X39 and X42, while another row has BB and BA, it may be relevant that in the first row, both start with X and both have two numeric digits, while the two values in the next row both have two uppercase alphabetic values, both starting with B. 

Where it is known that no columns represent ID or code values, these tests may be skipped, setting include_code_tests to False in check_data_quality(). As well, clear_results() may be called with clear_code_tests set to True to clear any results related to these tests if check_data_quality() is called including them, but subsequent evaluation finds no meaningful patterns of this type. The API get_tests_for_codes() may be called to get a list of the tests of this type.  

## Synthetic Data
The tool provides an API to generate synthetic data, which has been designed to provide columns that will be flagged by the tests, such that each test flags at least one column or set of columns. This may be useful for new users to help understand the tests, as they provide useful examples, and may be altered to determine when specifically the tests flag or do not flag patterns. The synthetic data is also used by the unit tests to help ensure consistent behaviour as the tests are expanded and improved. 
